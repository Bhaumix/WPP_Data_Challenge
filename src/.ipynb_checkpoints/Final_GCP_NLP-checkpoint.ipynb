{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "!pip install pandas tqdm requests httplib2 google-cloud-language\n",
    "clear_output()\n",
    "print('All Installed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httplib2\n",
    "import pandas as pd\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import datetime as dt\n",
    "import os\n",
    "import string\n",
    "from datetime import timedelta, date\n",
    "from urllib.error import HTTPError\n",
    "from IPython.display import clear_output\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from google.cloud import language\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "\n",
    "client = language.LanguageServiceClient.from_service_account_json('../resources/services.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_analysis(text):\n",
    "    content = text\n",
    "    document = types.Document(\n",
    "        content=content,\n",
    "        type=enums.Document.Type.PLAIN_TEXT)\n",
    "    \n",
    "    features = {'extract_syntax': True,\n",
    "                'extract_entities': True,\n",
    "                'extract_document_sentiment': True,\n",
    "                'extract_entity_sentiment': True,\n",
    "                'classify_text': False\n",
    "                }\n",
    "    response = client.annotate_text(document=document, features=features)\n",
    "    sentiment = response.document_sentiment\n",
    "    entities = response.entities\n",
    "    \n",
    "    response = client.classify_text(document)\n",
    "    categories = response.categories\n",
    "        \n",
    "    def get_type(type):\n",
    "        return client.enums.Entity.Type(entity.type).name\n",
    "    \n",
    "    invalid_types = ['OTHER']\n",
    "    result = {}\n",
    "    \n",
    "    result['sentiment'] = []    \n",
    "    result['entities'] = []\n",
    "    result['categories'] = []\n",
    "\n",
    "    if sentiment:\n",
    "        result['sentiment'] = [{ 'magnitude': sentiment.magnitude, 'score':sentiment.score }]\n",
    "        \n",
    "    for entity in entities:\n",
    "        if get_type(entity.type) not in invalid_types:\n",
    "            result['entities'].append({'name': entity.name, 'type': get_type(entity.type), 'salience': entity.salience, 'wikipedia_url': entity.metadata.get('wikipedia_url', '-')  })\n",
    "            \n",
    "    for category in categories:\n",
    "        result['categories'].append({'name':category.name, 'confidence': category.confidence})\n",
    "        \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/export_dataframe_cleaner1.csv\")\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"final_text1\"] = df[\"description\"] + df[\"What\"] + df[\"Cite\"] + df[\"Source Description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i, row in df.iterrows():\n",
    "    l = len(str(row.final_text1))\n",
    "    if len(str(row.final_text1).split()) > 20:\n",
    "        c=c+1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    result = ''\n",
    "    text = ''\n",
    "    text = str(row.final_text1)\n",
    "    #print(len(text))\n",
    "    #print(text)\n",
    "    if len(text.split()) > 20:\n",
    "        result = language_analysis(text)\n",
    "        #print(json.dumps(result, indent=4))\n",
    "        df.at[i, \"Text_Analysis\"] = json.dumps(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv ('/data/text_analysis.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
